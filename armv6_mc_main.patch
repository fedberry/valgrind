--- /root/rpmbuild/SOURCES/valgrind-3.11.0/memcheck/mc_main.c	2015-09-08 09:23:26.000000000 -0400
+++ valgrind-3.11.0/memcheck/mc_main.c	2016-11-11 10:53:23.207768806 -0500
@@ -4546,89 +4546,10 @@
    return mc_LOADV64(a, True);
 }
 
-// Non-generic assembly for arm32-linux
-#if ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-    && defined(VGP_arm_linux)
-__asm__( /* Derived from the 32 bit assembly helper */
-".text                                  \n"
-".align 2                               \n"
-".global vgMemCheck_helperc_LOADV64le   \n"
-".type   vgMemCheck_helperc_LOADV64le, %function \n"
-"vgMemCheck_helperc_LOADV64le:          \n"
-"      tst    r0, #7                    \n"
-"      movw   r3, #:lower16:primary_map \n"
-"      bne    .LLV64LEc4                \n" // if misaligned
-"      lsr    r2, r0, #16               \n"
-"      movt   r3, #:upper16:primary_map \n"
-"      ldr    r2, [r3, r2, lsl #2]      \n"
-"      uxth   r1, r0                    \n" // r1 is 0-(16)-0 X-(13)-X 000
-"      movw   r3, #0xAAAA               \n"
-"      lsr    r1, r1, #2                \n" // r1 is 0-(16)-0 00 X-(13)-X 0
-"      ldrh   r1, [r2, r1]              \n"
-"      cmp    r1, r3                    \n" // 0xAAAA == VA_BITS16_DEFINED
-"      bne    .LLV64LEc0                \n" // if !all_defined
-"      mov    r1, #0x0                  \n" // 0x0 == V_BITS32_DEFINED
-"      mov    r0, #0x0                  \n" // 0x0 == V_BITS32_DEFINED
-"      bx     lr                        \n"
-".LLV64LEc0:                            \n"
-"      movw   r3, #0x5555               \n"
-"      cmp    r1, r3                    \n" // 0x5555 == VA_BITS16_UNDEFINED
-"      bne    .LLV64LEc4                \n" // if !all_undefined
-"      mov    r1, #0xFFFFFFFF           \n" // 0xFFFFFFFF == V_BITS32_UNDEFINED
-"      mov    r0, #0xFFFFFFFF           \n" // 0xFFFFFFFF == V_BITS32_UNDEFINED
-"      bx     lr                        \n"
-".LLV64LEc4:                            \n"
-"      push   {r4, lr}                  \n"
-"      mov    r2, #0                    \n"
-"      mov    r1, #64                   \n"
-"      bl     mc_LOADVn_slow            \n"
-"      pop    {r4, pc}                  \n"
-".size vgMemCheck_helperc_LOADV64le, .-vgMemCheck_helperc_LOADV64le \n"
-".previous\n"
-);
-
-#elif ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-      && (defined(VGP_x86_linux) || defined(VGP_x86_solaris))
-__asm__(
-".text\n"
-".align 16\n"
-".global vgMemCheck_helperc_LOADV64le\n"
-".type   vgMemCheck_helperc_LOADV64le, @function\n"
-"vgMemCheck_helperc_LOADV64le:\n"
-"      test   $0x7,  %eax\n"
-"      jne    .LLV64LE2\n"          /* jump if not aligned */
-"      mov    %eax,  %ecx\n"
-"      movzwl %ax,   %edx\n"
-"      shr    $0x10, %ecx\n"
-"      mov    primary_map(,%ecx,4), %ecx\n"
-"      shr    $0x3,  %edx\n"
-"      movzwl (%ecx,%edx,2), %edx\n"
-"      cmp    $0xaaaa, %edx\n"
-"      jne    .LLV64LE1\n"          /* jump if not all defined */
-"      xor    %eax, %eax\n"         /* return 0 in edx:eax */
-"      xor    %edx, %edx\n"
-"      ret\n"
-".LLV64LE1:\n"
-"      cmp    $0x5555, %edx\n"
-"      jne    .LLV64LE2\n"         /* jump if not all undefined */
-"      or     $0xffffffff, %eax\n" /* else return all bits set in edx:eax */
-"      or     $0xffffffff, %edx\n"
-"      ret\n"
-".LLV64LE2:\n"
-"      xor    %ecx,  %ecx\n"  /* tail call to mc_LOADVn_slow(a, 64, 0) */
-"      mov    $64,   %edx\n"
-"      jmp    mc_LOADVn_slow\n"
-".size vgMemCheck_helperc_LOADV64le, .-vgMemCheck_helperc_LOADV64le\n"
-".previous\n"
-);
-
-#else
-// Generic for all platforms except {arm32,x86}-linux and x86-solaris
 VG_REGPARM(1) ULong MC_(helperc_LOADV64le) ( Addr a )
 {
    return mc_LOADV64(a, False);
 }
-#endif
 
 /*------------------------------------------------------------*/
 /*--- STOREV64                                             ---*/
@@ -4749,82 +4670,11 @@
    return mc_LOADV32(a, True);
 }
 
-// Non-generic assembly for arm32-linux
-#if ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-    && defined(VGP_arm_linux)
-__asm__( /* Derived from NCode template */
-".text                                  \n"
-".align 2                               \n"
-".global vgMemCheck_helperc_LOADV32le   \n"
-".type   vgMemCheck_helperc_LOADV32le, %function \n"
-"vgMemCheck_helperc_LOADV32le:          \n"
-"      tst    r0, #3                    \n" // 1
-"      movw   r3, #:lower16:primary_map \n" // 1
-"      bne    .LLV32LEc4                \n" // 2  if misaligned
-"      lsr    r2, r0, #16               \n" // 3
-"      movt   r3, #:upper16:primary_map \n" // 3
-"      ldr    r2, [r3, r2, lsl #2]      \n" // 4
-"      uxth   r1, r0                    \n" // 4
-"      ldrb   r1, [r2, r1, lsr #2]      \n" // 5
-"      cmp    r1, #0xAA                 \n" // 6  0xAA == VA_BITS8_DEFINED
-"      bne    .LLV32LEc0                \n" // 7  if !all_defined
-"      mov    r0, #0x0                  \n" // 8  0x0 == V_BITS32_DEFINED
-"      bx     lr                        \n" // 9
-".LLV32LEc0:                            \n"
-"      cmp    r1, #0x55                 \n" // 0x55 == VA_BITS8_UNDEFINED
-"      bne    .LLV32LEc4                \n" // if !all_undefined
-"      mov    r0, #0xFFFFFFFF           \n" // 0xFFFFFFFF == V_BITS32_UNDEFINED
-"      bx     lr                        \n"
-".LLV32LEc4:                            \n"
-"      push   {r4, lr}                  \n"
-"      mov    r2, #0                    \n"
-"      mov    r1, #32                   \n"
-"      bl     mc_LOADVn_slow            \n"
-"      pop    {r4, pc}                  \n"
-".size vgMemCheck_helperc_LOADV32le, .-vgMemCheck_helperc_LOADV32le \n"
-".previous\n"
-);
-
-#elif ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-      && (defined(VGP_x86_linux) || defined(VGP_x86_solaris))
-__asm__(
-".text\n"
-".align 16\n"
-".global vgMemCheck_helperc_LOADV32le\n"
-".type   vgMemCheck_helperc_LOADV32le, @function\n"
-"vgMemCheck_helperc_LOADV32le:\n"
-"      test   $0x3,  %eax\n"
-"      jnz    .LLV32LE2\n"         /* jump if misaligned */
-"      mov    %eax,  %edx\n"
-"      shr    $16,   %edx\n"
-"      mov    primary_map(,%edx,4), %ecx\n"
-"      movzwl %ax,   %edx\n"
-"      shr    $2,    %edx\n"
-"      movzbl (%ecx,%edx,1), %edx\n"
-"      cmp    $0xaa, %edx\n"       /* compare to VA_BITS8_DEFINED */
-"      jne    .LLV32LE1\n"         /* jump if not completely defined */
-"      xor    %eax,  %eax\n"       /* else return V_BITS32_DEFINED */
-"      ret\n"
-".LLV32LE1:\n"
-"      cmp    $0x55, %edx\n"       /* compare to VA_BITS8_UNDEFINED */
-"      jne    .LLV32LE2\n"         /* jump if not completely undefined */
-"      or     $0xffffffff, %eax\n" /* else return V_BITS32_UNDEFINED */
-"      ret\n"
-".LLV32LE2:\n"
-"      xor    %ecx,  %ecx\n"       /* tail call mc_LOADVn_slow(a, 32, 0) */
-"      mov    $32,   %edx\n"
-"      jmp    mc_LOADVn_slow\n"
-".size vgMemCheck_helperc_LOADV32le, .-vgMemCheck_helperc_LOADV32le\n"
-".previous\n"
-);
-
-#else
 // Generic for all platforms except {arm32,x86}-linux and x86-solaris
 VG_REGPARM(1) UWord MC_(helperc_LOADV32le) ( Addr a )
 {
    return mc_LOADV32(a, False);
 }
-#endif
 
 /*------------------------------------------------------------*/
 /*--- STOREV32                                             ---*/
@@ -4945,111 +4795,11 @@
    return mc_LOADV16(a, True);
 }
 
-// Non-generic assembly for arm32-linux
-#if ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-    && defined(VGP_arm_linux)
-__asm__( /* Derived from NCode template */
-".text                                  \n"
-".align 2                               \n"
-".global vgMemCheck_helperc_LOADV16le   \n"
-".type   vgMemCheck_helperc_LOADV16le, %function \n"
-"vgMemCheck_helperc_LOADV16le:          \n" //
-"      tst    r0, #1                    \n" // 
-"      bne    .LLV16LEc12               \n" // if misaligned
-"      lsr    r2, r0, #16               \n" // r2 = pri-map-ix
-"      movw   r3, #:lower16:primary_map \n" //
-"      uxth   r1, r0                    \n" // r1 = sec-map-offB
-"      movt   r3, #:upper16:primary_map \n" //
-"      ldr    r2, [r3, r2, lsl #2]      \n" // r2 = sec-map
-"      ldrb   r1, [r2, r1, lsr #2]      \n" // r1 = sec-map-VABITS8
-"      cmp    r1, #0xAA                 \n" // r1 == VA_BITS8_DEFINED?
-"      bne    .LLV16LEc0                \n" // no, goto .LLV16LEc0
-".LLV16LEh9:                            \n" //
-"      mov    r0, #0xFFFFFFFF           \n" //
-"      lsl    r0, r0, #16               \n" // V_BITS16_DEFINED | top16safe
-"      bx     lr                        \n" //
-".LLV16LEc0:                            \n" //
-"      cmp    r1, #0x55                 \n" // VA_BITS8_UNDEFINED
-"      bne    .LLV16LEc4                \n" //
-".LLV16LEc2:                            \n" //
-"      mov    r0, #0xFFFFFFFF           \n" // V_BITS16_UNDEFINED | top16safe
-"      bx     lr                        \n" //
-".LLV16LEc4:                            \n" //
-       // r1 holds sec-map-VABITS8.  r0 holds the address and is 2-aligned.
-       // Extract the relevant 4 bits and inspect.
-"      and    r2, r0, #2       \n" // addr & 2
-"      add    r2, r2, r2       \n" // 2 * (addr & 2)
-"      lsr    r1, r1, r2       \n" // sec-map-VABITS8 >> (2 * (addr & 2))
-"      and    r1, r1, #15      \n" // (sec-map-VABITS8 >> (2 * (addr & 2))) & 15
-
-"      cmp    r1, #0xA                  \n" // VA_BITS4_DEFINED
-"      beq    .LLV16LEh9                \n" //
-
-"      cmp    r1, #0x5                  \n" // VA_BITS4_UNDEFINED
-"      beq    .LLV16LEc2                \n" //
-
-".LLV16LEc12:                           \n" //
-"      push   {r4, lr}                  \n" //
-"      mov    r2, #0                    \n" //
-"      mov    r1, #16                   \n" //
-"      bl     mc_LOADVn_slow            \n" //
-"      pop    {r4, pc}                  \n" //
-".size vgMemCheck_helperc_LOADV16le, .-vgMemCheck_helperc_LOADV16le \n"
-".previous\n"
-);
-
-#elif ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-      && (defined(VGP_x86_linux) || defined(VGP_x86_solaris))
-__asm__(
-".text\n"
-".align 16\n"
-".global vgMemCheck_helperc_LOADV16le\n"
-".type   vgMemCheck_helperc_LOADV16le, @function\n"
-"vgMemCheck_helperc_LOADV16le:\n"
-"      test   $0x1,  %eax\n"
-"      jne    .LLV16LE5\n"          /* jump if not aligned */
-"      mov    %eax,  %edx\n"
-"      shr    $0x10, %edx\n"
-"      mov    primary_map(,%edx,4), %ecx\n"
-"      movzwl %ax,   %edx\n"
-"      shr    $0x2,  %edx\n"
-"      movzbl (%ecx,%edx,1), %edx\n"/* edx = VA bits for 32bit */
-"      cmp    $0xaa, %edx\n"        /* compare to VA_BITS8_DEFINED */
-"      jne    .LLV16LE2\n"          /* jump if not all 32bits defined */
-".LLV16LE1:\n"
-"      mov    $0xffff0000,%eax\n"   /* V_BITS16_DEFINED | top16safe */
-"      ret\n"
-".LLV16LE2:\n"
-"      cmp    $0x55, %edx\n"        /* compare to VA_BITS8_UNDEFINED */
-"      jne    .LLV16LE4\n"          /* jump if not all 32bits undefined */
-".LLV16LE3:\n"
-"      or     $0xffffffff,%eax\n"   /* V_BITS16_UNDEFINED | top16safe */
-"      ret\n"
-".LLV16LE4:\n"
-"      mov    %eax,  %ecx\n"
-"      and    $0x2,  %ecx\n"
-"      add    %ecx,  %ecx\n"
-"      sar    %cl,   %edx\n"
-"      and    $0xf,  %edx\n"
-"      cmp    $0xa,  %edx\n"
-"      je     .LLV16LE1\n"          /* jump if all 16bits are defined */
-"      cmp    $0x5,  %edx\n"
-"      je     .LLV16LE3\n"          /* jump if all 16bits are undefined */
-".LLV16LE5:\n"
-"      xor    %ecx,  %ecx\n"        /* tail call mc_LOADVn_slow(a, 16, 0) */
-"      mov    $16,   %edx\n"
-"      jmp    mc_LOADVn_slow\n"
-".size vgMemCheck_helperc_LOADV16le, .-vgMemCheck_helperc_LOADV16le \n"
-".previous\n"
-);
-
-#else
 // Generic for all platforms except {arm32,x86}-linux and x86-solaris
 VG_REGPARM(1) UWord MC_(helperc_LOADV16le) ( Addr a )
 {
    return mc_LOADV16(a, False);
 }
-#endif
 
 /*------------------------------------------------------------*/
 /*--- STOREV16                                             ---*/
@@ -5142,99 +4892,6 @@
 
 /* Note: endianness is irrelevant for size == 1 */
 
-// Non-generic assembly for arm32-linux
-#if ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-    && defined(VGP_arm_linux)
-__asm__( /* Derived from NCode template */
-".text                                  \n"
-".align 2                               \n"
-".global vgMemCheck_helperc_LOADV8      \n"
-".type   vgMemCheck_helperc_LOADV8, %function \n"
-"vgMemCheck_helperc_LOADV8:             \n" //
-"      lsr    r2, r0, #16               \n" // r2 = pri-map-ix
-"      movw   r3, #:lower16:primary_map \n" //
-"      uxth   r1, r0                    \n" // r1 = sec-map-offB
-"      movt   r3, #:upper16:primary_map \n" //
-"      ldr    r2, [r3, r2, lsl #2]      \n" // r2 = sec-map
-"      ldrb   r1, [r2, r1, lsr #2]      \n" // r1 = sec-map-VABITS8
-"      cmp    r1, #0xAA                 \n" // r1 == VA_BITS8_DEFINED?
-"      bne    .LLV8c0                   \n" // no, goto .LLV8c0
-".LLV8h9:                               \n" //
-"      mov    r0, #0xFFFFFF00           \n" // V_BITS8_DEFINED | top24safe
-"      bx     lr                        \n" //
-".LLV8c0:                               \n" //
-"      cmp    r1, #0x55                 \n" // VA_BITS8_UNDEFINED
-"      bne    .LLV8c4                   \n" //
-".LLV8c2:                               \n" //
-"      mov    r0, #0xFFFFFFFF           \n" // V_BITS8_UNDEFINED | top24safe
-"      bx     lr                        \n" //
-".LLV8c4:                               \n" //
-       // r1 holds sec-map-VABITS8
-       // r0 holds the address.  Extract the relevant 2 bits and inspect.
-"      and    r2, r0, #3       \n" // addr & 3
-"      add    r2, r2, r2       \n" // 2 * (addr & 3)
-"      lsr    r1, r1, r2       \n" // sec-map-VABITS8 >> (2 * (addr & 3))
-"      and    r1, r1, #3       \n" // (sec-map-VABITS8 >> (2 * (addr & 3))) & 3
-
-"      cmp    r1, #2                    \n" // VA_BITS2_DEFINED
-"      beq    .LLV8h9                   \n" //
-
-"      cmp    r1, #1                    \n" // VA_BITS2_UNDEFINED
-"      beq    .LLV8c2                   \n" //
-
-"      push   {r4, lr}                  \n" //
-"      mov    r2, #0                    \n" //
-"      mov    r1, #8                    \n" //
-"      bl     mc_LOADVn_slow            \n" //
-"      pop    {r4, pc}                  \n" //
-".size vgMemCheck_helperc_LOADV8, .-vgMemCheck_helperc_LOADV8 \n"
-".previous\n"
-);
-
-/* Non-generic assembly for x86-linux */
-#elif ENABLE_ASSEMBLY_HELPERS && defined(PERF_FAST_LOADV) \
-      && (defined(VGP_x86_linux) || defined(VGP_x86_solaris))
-__asm__(
-".text\n"
-".align 16\n"
-".global vgMemCheck_helperc_LOADV8\n"
-".type   vgMemCheck_helperc_LOADV8, @function\n"
-"vgMemCheck_helperc_LOADV8:\n"
-"      mov    %eax,  %edx\n"
-"      shr    $0x10, %edx\n"
-"      mov    primary_map(,%edx,4), %ecx\n"
-"      movzwl %ax,   %edx\n"
-"      shr    $0x2,  %edx\n"
-"      movzbl (%ecx,%edx,1), %edx\n"/* edx = VA bits for 32bit */
-"      cmp    $0xaa, %edx\n"        /* compare to VA_BITS8_DEFINED? */
-"      jne    .LLV8LE2\n"           /* jump if not defined */
-".LLV8LE1:\n"
-"      mov    $0xffffff00, %eax\n"  /* V_BITS8_DEFINED | top24safe */
-"      ret\n"
-".LLV8LE2:\n"
-"      cmp    $0x55, %edx\n"        /* compare to VA_BITS8_UNDEFINED */
-"      jne    .LLV8LE4\n"           /* jump if not all 32bits are undefined */
-".LLV8LE3:\n"
-"      or     $0xffffffff, %eax\n"  /* V_BITS8_UNDEFINED | top24safe */
-"      ret\n"
-".LLV8LE4:\n"
-"      mov    %eax,  %ecx\n"
-"      and    $0x3,  %ecx\n"
-"      add    %ecx,  %ecx\n"
-"      sar    %cl,   %edx\n"
-"      and    $0x3,  %edx\n"
-"      cmp    $0x2,  %edx\n"
-"      je     .LLV8LE1\n"           /* jump if all 8bits are defined */
-"      cmp    $0x1,  %edx\n"
-"      je     .LLV8LE3\n"           /* jump if all 8bits are undefined */
-"      xor    %ecx,  %ecx\n"        /* tail call to mc_LOADVn_slow(a, 8, 0) */
-"      mov    $0x8,  %edx\n"
-"      jmp    mc_LOADVn_slow\n"
-".size vgMemCheck_helperc_LOADV8, .-vgMemCheck_helperc_LOADV8\n"
-".previous\n"
-);
-
-#else
 // Generic for all platforms except {arm32,x86}-linux and x86-solaris
 VG_REGPARM(1)
 UWord MC_(helperc_LOADV8) ( Addr a )
@@ -5276,7 +4933,6 @@
    }
 #endif
 }
-#endif
 
 /*------------------------------------------------------------*/
 /*--- STOREV8                                              ---*/
